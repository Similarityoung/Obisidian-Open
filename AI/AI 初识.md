---
title: AI 初识
tags:
  - func
  - llm
categories:
  - AI
date: 2025-08-24T19:15:28+08:00
draft: true
---
### 1. 函数

#### 最初的尝试与瓶颈：精确函数的局限

一切的起点是函数 (Function)：给定一个输入 `x`，通过一个明确的规则 `f`，得到一个输出 `y`。

对于简单的问题，比如 `y = 2x + 1`，这个规则很容易找到。但现实世界的问题要复杂得多。例如，在图像识别中，输入 `x` 是成千上万个像素点，输出 `y` 是一个简单的词，比如“猫”。想要写出一个唯一的、完美的函数 `f`，能一步到位地将百万像素与“猫”这个概念精确地联系起来，这几乎是不可能完成的任务。

#### 核心思路的转变：拥抱近似解

既然寻找完美的“精确解”这条路走不通，研究者们便转变了思路：放弃追求100%正确的唯一答案，转而寻找一个“大差不差”的**近似解 (Approximate Solution)**。

这个思路的转变是革命性的。它意味着我们不再试图去理解那个完美的、神一样的函数到底是什么，而是尝试去“模仿”它、“逼近”它。我们的目标变成了：构建一个系统，只要它的输出结果在实际应用中足够好，我们就接受它。

#### 实现方式：线性与非线性的组合

那么，如何去构建这个“近似解”呢？答案是通过两种基础工具的巧妙组合：

- **线性拟合 (Linear Fitting)**：这是最基础的构建模块，形式为 `y = Wx + B`。你可以把它想象成一把“直尺”，它负责在复杂的数据关系中，尽力画出一条最贴近的直线，完成一次最简单的局部近似。

- **激活函数 (Activation Function)**：现实世界的关系是弯弯曲曲的，光靠“直尺”是不够的。激活函数就像一个“扳手”，它负责在线性拟合画出的直线上增加一个“拐点”，把它“掰弯”，从而赋予模型描绘非线性关系的能力。


将这两个步骤合在一起——先用线性函数尽力拟合，再用激活函数进行非线性转换——就构成了一个最基本的处理单元，也就是**神经元 (Neuron)**。

#### 直观表示与最终目标：神经网络

一个神经元只能进行一次简单的“掰弯”，这对于复杂的现实问题是远远不够的。因此，我们把成千上万个神经元组织起来，像搭积木一样层层堆叠。前一层神经元的输出，作为后一层神经元的输入。

这种由大量神经元组成的、层层递进的庞大结构，就是**神经网络 (Neural Network)**。

这个网络的最终目标，就是通过“学习”大量的已知数据（这个过程叫“训练”），来自动调整网络中每一个神经元内部的参数 **W (权重)** 和 **B (偏置)**。当找到一组最优的W和B时，这个网络就具备了强大的拟合能力，能够接收一个全新的输入 `x`，并计算出一个非常接近真实答案的 `y`

