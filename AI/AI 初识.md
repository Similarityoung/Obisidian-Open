---
title: AI 初识
tags:
  - func
  - llm
categories:
  - AI
date: 2025-08-24T19:15:28+08:00
draft: true
---
### 1. 从函数函数到神经网络

#### 最初的尝试与瓶颈：精确函数的局限

一切的起点是函数 (Function)：给定一个输入 `x`，通过一个明确的规则 `f`，得到一个输出 `y`。

对于简单的问题，比如 `y = 2x + 1`，这个规则很容易找到。但现实世界的问题要复杂得多。例如，在图像识别中，输入 `x` 是成千上万个像素点，输出 `y` 是一个简单的词，比如“猫”。想要写出一个唯一的、完美的函数 `f`，能一步到位地将百万像素与“猫”这个概念精确地联系起来，这几乎是不可能完成的任务。

#### 核心思路的转变：拥抱近似解

既然寻找完美的“精确解”这条路走不通，研究者们便转变了思路：放弃追求100%正确的唯一答案，转而寻找一个“大差不差”的**近似解 (Approximate Solution)**。

这个思路的转变是革命性的。它意味着我们不再试图去理解那个完美的、神一样的函数到底是什么，而是尝试去“模仿”它、“逼近”它。我们的目标变成了：构建一个系统，只要它的输出结果在实际应用中足够好，我们就接受它。

#### 实现方式：线性与非线性的组合

那么，如何去构建这个“近似解”呢？答案是通过两种基础工具的巧妙组合：

- **线性拟合 (Linear Fitting)**：这是最基础的构建模块，形式为 `y = Wx + B`。你可以把它想象成一把“直尺”，它负责在复杂的数据关系中，尽力画出一条最贴近的直线，完成一次最简单的局部近似。

- **激活函数 (Activation Function)**：现实世界的关系是弯弯曲曲的，光靠“直尺”是不够的。激活函数就像一个“扳手”，它负责在线性拟合画出的直线上增加一个“拐点”，把它“掰弯”，从而赋予模型描绘非线性关系的能力。


将这两个步骤合在一起——先用线性函数尽力拟合，再用激活函数进行非线性转换——就构成了一个最基本的处理单元，也就是**神经元 (Neuron)**。

#### 直观表示与最终目标：神经网络

一个神经元只能进行一次简单的“掰弯”，这对于复杂的现实问题是远远不够的。因此，我们把成千上万个神经元组织起来，像搭积木一样层层堆叠。前一层神经元的输出，作为后一层神经元的输入。

这种由大量神经元组成的、层层递进的庞大结构，就是**神经网络 (Neural Network)**。

这个网络的最终目标，就是通过“学习”大量的已知数据（这个过程叫“训练”），来自动调整网络中每一个神经元内部的参数 **W (权重)** 和 **B (偏置)**。当找到一组最优的W和B时，这个网络就具备了强大的拟合能力，能够接收一个全新的输入 `x`，并计算出一个非常接近真实答案的 `y`

### 2. 计算神经网络的参数

#### 目标与困境：最小化损失

训练神经网络的根本目标，是调整内部的参数（W和B），使得网络的预测结果与真实答案尽可能地接近。为了衡量这种“接近”的程度，我们引入了**损失函数 (Loss Function)**。

- **损失函数**：一个数学函数，用来计算“预测值”与“真实值”之间的差距。差距越大，损失函数的值就越大。

- **我们的目标**：找到一组完美的W和B，使得损失函数的值达到**最小**。


然而，这里的困境在于，一个大型神经网络的结构极其复杂，其损失函数也是一个包含了成千上万个变量的、极其复杂的函数。我们无法像解简单的方程那样，通过数学公式直接计算出能让损失函数最小的W和B（即无法求得**解析解**）。

#### 核心方法：梯度下降法

既然无法一步到位，我们就换一种思路：**逐步逼近**。这就是**梯度下降法 (Gradient Descent)**的核心思想。

我们可以把寻找最小损失值的过程，想象成一个人在黑夜里站在一座陡峭的山上，想要尽快走到山谷的最低点。

- **当前位置**：代表了当前参数（W和B）下的损失值。

- **目标**：走到山谷最低点（损失函数的最小值）。


在黑暗中，最明智的策略就是：**在当前站立的位置，伸脚向四周探索，找到最陡峭的下坡方向，然后朝着这个方向迈一小步。** 不断重复这个过程，虽然不确定能一步走到最低点，但可以保证每一步都在朝着更低的地方走。

这个“最陡峭的下坡方向”，在数学上就是损失函数在该点的**梯度 (Gradient)** 的反方向。梯度本身指向函数值增长最快的方向，那么它的反方向自然就是函数值下降最快的方向。

#### 关键技术：反向传播

梯度下降的思路很清晰，但在庞大的神经网络中，计算这个“梯度”本身就是一个难题。因为损失函数位于整个网络计算流程的末端，而参数W和B分散在网络的各个层级中。直接去求损失函数对某个深层参数的偏导数，会非常困难。

为了解决这个问题，我们使用了**链式法则 (Chain Rule)**，这就是**反向传播 (Backpropagation)** 算法的数学基础。

它的工作流程如下：

1. **前向传播 (Forward Propagation)**：首先，我们让数据从输入层开始，一路通过整个网络计算，直到在输出层得到一个预测结果，并根据这个结果计算出当前的损失值。

2. **反向传播 (Backpropagation)**：然后，我们从损失函数开始，**反向地**、**逐层地**将“误差”传播回去。利用链式法则，我们可以先计算出输出层对损失的梯度，然后利用这个结果去计算倒数第二层的梯度，再往前……直到计算出网络中每一个参数W和B对最终损失的梯度。


通过不断重复“**前向传播 → 计算损失 → 反向传播 → 更新参数**”这个循环，神经网络中的参数W和B就会被不断地微调，使得损失值越来越小，最终让整个网络越来越“聪明”。