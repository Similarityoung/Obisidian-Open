---
title: AI 初识
tags:
  - func
  - llm
categories:
  - AI
date: 2025-08-24T19:15:28+08:00
draft: true
---
### 1. 从函数函数到神经网络

#### 最初的尝试与瓶颈：精确函数的局限

一切的起点是函数 (Function)：给定一个输入 `x`，通过一个明确的规则 `f`，得到一个输出 `y`。

对于简单的问题，比如 `y = 2x + 1`，这个规则很容易找到。但现实世界的问题要复杂得多。例如，在图像识别中，输入 `x` 是成千上万个像素点，输出 `y` 是一个简单的词，比如“猫”。想要写出一个唯一的、完美的函数 `f`，能一步到位地将百万像素与“猫”这个概念精确地联系起来，这几乎是不可能完成的任务。

#### 核心思路的转变：拥抱近似解

既然寻找完美的“精确解”这条路走不通，研究者们便转变了思路：放弃追求100%正确的唯一答案，转而寻找一个“大差不差”的**近似解 (Approximate Solution)**。

这个思路的转变是革命性的。它意味着我们不再试图去理解那个完美的、神一样的函数到底是什么，而是尝试去“模仿”它、“逼近”它。我们的目标变成了：构建一个系统，只要它的输出结果在实际应用中足够好，我们就接受它。

#### 实现方式：线性与非线性的组合

那么，如何去构建这个“近似解”呢？答案是通过两种基础工具的巧妙组合：

- **线性拟合 (Linear Fitting)**：这是最基础的构建模块，形式为 `y = Wx + B`。你可以把它想象成一把“直尺”，它负责在复杂的数据关系中，尽力画出一条最贴近的直线，完成一次最简单的局部近似。

- **激活函数 (Activation Function)**：现实世界的关系是弯弯曲曲的，光靠“直尺”是不够的。激活函数就像一个“扳手”，它负责在线性拟合画出的直线上增加一个“拐点”，把它“掰弯”，从而赋予模型描绘非线性关系的能力。


将这两个步骤合在一起——先用线性函数尽力拟合，再用激活函数进行非线性转换——就构成了一个最基本的处理单元，也就是**神经元 (Neuron)**。

#### 直观表示与最终目标：神经网络

一个神经元只能进行一次简单的“掰弯”，这对于复杂的现实问题是远远不够的。因此，我们把成千上万个神经元组织起来，像搭积木一样层层堆叠。前一层神经元的输出，作为后一层神经元的输入。

这种由大量神经元组成的、层层递进的庞大结构，就是**神经网络 (Neural Network)**。

这个网络的最终目标，就是通过“学习”大量的已知数据（这个过程叫“训练”），来自动调整网络中每一个神经元内部的参数 **W (权重)** 和 **B (偏置)**。当找到一组最优的W和B时，这个网络就具备了强大的拟合能力，能够接收一个全新的输入 `x`，并计算出一个非常接近真实答案的 `y`

### 2. 计算神经网络的参数

#### 目标与困境：最小化损失

训练神经网络的根本目标，是调整内部的参数（W和B），使得网络的预测结果与真实答案尽可能地接近。为了衡量这种“接近”的程度，我们引入了**损失函数 (Loss Function)**。

- **损失函数**：一个数学函数，用来计算“预测值”与“真实值”之间的差距。差距越大，损失函数的值就越大。

- **我们的目标**：找到一组完美的W和B，使得损失函数的值达到**最小**。


然而，这里的困境在于，一个大型神经网络的结构极其复杂，其损失函数也是一个包含了成千上万个变量的、极其复杂的函数。我们无法像解简单的方程那样，通过数学公式直接计算出能让损失函数最小的W和B（即无法求得**解析解**）。

#### 核心方法：梯度下降法

既然无法一步到位，我们就换一种思路：**逐步逼近**。这就是**梯度下降法 (Gradient Descent)**的核心思想。

我们可以把寻找最小损失值的过程，想象成一个人在黑夜里站在一座陡峭的山上，想要尽快走到山谷的最低点。

- **当前位置**：代表了当前参数（W和B）下的损失值。

- **目标**：走到山谷最低点（损失函数的最小值）。


在黑暗中，最明智的策略就是：**在当前站立的位置，伸脚向四周探索，找到最陡峭的下坡方向，然后朝着这个方向迈一小步。** 不断重复这个过程，虽然不确定能一步走到最低点，但可以保证每一步都在朝着更低的地方走。

这个“最陡峭的下坡方向”，在数学上就是损失函数在该点的**梯度 (Gradient)** 的反方向。梯度本身指向函数值增长最快的方向，那么它的反方向自然就是函数值下降最快的方向。

#### 关键技术：反向传播

梯度下降的思路很清晰，但在庞大的神经网络中，计算这个“梯度”本身就是一个难题。因为损失函数位于整个网络计算流程的末端，而参数W和B分散在网络的各个层级中。直接去求损失函数对某个深层参数的偏导数，会非常困难。

为了解决这个问题，我们使用了**链式法则 (Chain Rule)**，这就是**反向传播 (Backpropagation)** 算法的数学基础。

它的工作流程如下：

1. **前向传播 (Forward Propagation)**：首先，我们让数据从输入层开始，一路通过整个网络计算，直到在输出层得到一个预测结果，并根据这个结果计算出当前的损失值。

2. **反向传播 (Backpropagation)**：然后，我们从损失函数开始，**反向地**、**逐层地**将“误差”传播回去。利用链式法则，我们可以先计算出输出层对损失的梯度，然后利用这个结果去计算倒数第二层的梯度，再往前……直到计算出网络中每一个参数W和B对最终损失的梯度。


通过不断重复“**前向传播 → 计算损失 → 反向传播 → 更新参数**”这个循环，神经网络中的参数W和B就会被不断地微调，使得损失值越来越小，最终让整个网络越来越“聪明”。

### 3. 调整神经网络的方法

> 函数过拟合 => 泛化能力差，解决方法：
> 
> 1. 根据原有的数据创造更多的数据：翻转，裁剪，加噪声 => 不会因为很小的扰动产生很大的变化 => 增强模型鲁棒性
> 
> 2. 提前中断训练；损失函数添加惩罚项，即正则化；dropout，随机丢弃某些参数
> 
> 拓展了解：梯度消失，梯度爆炸，收敛速度，计算开销；
> 梯度裁剪，残差网络，权重初始化，归一化，动量法，RMSProp，Adam，mini-batch

#### 核心挑战：过拟合 (Overfitting)

在训练过程中，我们可能会遇到一个常见的问题，叫做**过拟合**。

- **什么是过拟合？** 简单来说，就是模型对训练数据学习得“太好”了，以至于把数据中的一些噪声和偶然特征也当作了通用规律。这就像一个学生只会死记硬背题库，一遇到新题型就不会变通了。

- **后果：** 过拟合的模型在面对新的、未见过的数据时表现会很差，也就是**泛化能力差**。

#### 应对策略：如何提升模型的泛化能力

##### A. 从数据本身出发：数据增强 (Data Augmentation)

既然模型是因为数据不够多样而“想太多”，那么最直接的方法就是增加数据的多样性。根据已有的数据，创造出更多“新的”数据。**常用技巧**：图像翻转/裁剪：改变图像的方向和视角。**增加噪声：** 在数据中加入微小的随机扰动。

这样做能让模型学习到更本质的特征，而不会因为一些微小的变化（如光线、角度）就产生错误的判断，从而增强模型的**鲁棒性 (Robustness)**。

##### B. 从模型和训练过程入手：正则化与Dropout

除了扩充数据，我们还可以通过修改模型本身或训练方法来限制其“死记硬背”的能力。

- **提前中断训练 (Early Stopping)：** 在模型于验证集上的表现开始下降时，及时停止训练，防止它在训练数据上走得“太远”。
    
- **正则化 (Regularization)：** 在损失函数中添加一个“惩罚项”，这个惩罚项会限制模型参数（W）的大小。参数值越大，惩罚就越重。这迫使模型去学习更简单、更平滑的模式，而不是用复杂的参数去拟合每一个数据点。
    
- **Dropout：** 在训练的每一步，都随机地“丢弃”一部分神经元（让它们暂时不工作）。这强迫网络不能过度依赖任何一个神经元，而是让每个神经元都学习到一些有用的特征，从而提升了模型的健壮性。

#### 拓展了解：其他关键的优化技巧

在神经网络的训练过程中，除了过拟合，我们还会关心其他问题，比如训练的稳定性和效率。以下是一些常见的高级优化技巧：

- **问题与挑战：**
    
    - **梯度消失/梯度爆炸 (Vanishing/Exploding Gradients)：** 在深层网络中，梯度在反向传播时可能变得过小或过大，导致训练困难。
        
    - **收敛速度与计算开销：** 如何让模型更快、更有效地达到最佳状态。
        
- **常用解决方案：**
    
    - **梯度裁剪 (Gradient Clipping)：** 为梯度设置一个上限，防止其爆炸。
        
    - **残差网络 (ResNet)：** 一种特殊的网络结构，能有效缓解梯度消失问题。
        
    - **权重初始化 (Weight Initialization)：** 采用合适的初始参数值，让训练有一个好的起点。
        
    - **归一化 (Normalization)：** 将数据调整到相似的尺度，加速收敛。
        
    - **高级优化器：**
        
        - **动量法 (Momentum)：** 引入“惯性”概念，帮助梯度下降冲出局部最优。
            
        - **RMSProp / Adam：** 能够自适应地调整每个参数的学习率，是目前最常用的优化器之一。
            
    - **Mini-batch Gradient Descent：** 每次只用一小批数据来计算梯度和更新参数，兼顾了计算效率和更新的稳定性。

### 4. 从矩阵到 CNN

>  矩阵代替函数 => 表达方便，利用 GPU 效率高
> 
> 全连接FC具有局限性 => 卷积代替标准矩阵运损 => 减少训练参数，更好提取特征 => 由全连接层拓展到卷积层，池化层，全连接层。 => 卷积神经网络 CNN

#### 基础：神经网络与矩阵运算

传统的神经网络，尤其是**全连接层 (Fully Connected Layer, FC)**，其核心可以用**矩阵运算**来表示。

**为什么要用矩阵？**

- **表达方便：** 可以将复杂的网络连接和参数（W和B）简洁地表示出来。
    
- **计算高效：** 现代计算硬件（尤其是 GPU）对矩阵运算进行了高度优化，可以进行大规模的并行计算，极大地提升了训练效率。

#### 全连接层的局限性

尽管全连接层很强大，但在处理图像这类高维度数据时，会遇到一些瓶颈：

- **参数量巨大：** 在全连接层中，每一个输入神经元都与每一个输出神经元相连。如果输入的是一张高分辨率的图片（例如1000x1000像素），那么第一层的参数数量就会变得异常庞大，导致模型训练极其困难且容易过拟合。

- **特征提取能力有限：** 全连接层将图像数据“展平”成一个长向量来处理，这破坏了图像本身的空间结构信息（例如，哪些像素是相邻的）。它很难有效地提取出图像中的局部特征（如边缘、角点、纹理）。

#### 卷积运算的引入

为了解决上述问题，研究者们引入了**卷积 (Convolution)** 运算来替代全连接层中的标准矩阵运算。

**核心思想:**

- **参数共享：** 使用一个很小的“卷积核”（可以看作一个迷你的、可学习的特征探测器），让它在整张图片上滑动扫描。这个卷积核的参数是共享的，无论它移动到图片哪个位置，参数都保持不变。这极大地**减少了需要训练的参数数量**。

- **局部特征提取：** 由于卷积核每次只关注图像的一小块区域，它非常擅长捕捉图像的局部特征。不同的卷积核可以学习到不同的特征（例如，有的负责找横线，有的负责找竖线）。

#### 卷积神经网络 (CNN) 的诞生

通过将传统的全连接层拓展和重组，就形成了现代的**卷积神经网络 (CNN)**。一个典型的CNN结构通常包含以下几个关键部分：

1. **卷积层 (Convolutional Layer)：** 负责用卷积核提取图像的局部特征。

2. **池化层 (Pooling Layer)：** 负责对特征图进行“压缩”，减少数据量，同时保留最重要的特征信息，进一步减少计算开销。

3. **全连接层 (Fully Connected Layer)：** 在网络的最后，通常会接上一到两个全连接层，负责将前面提取到的各种特征进行整合，并最终完成分类或识别任务。